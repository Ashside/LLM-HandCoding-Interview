# LLM-HandCoding-Interview
收集为大模型面试准备的手撕代码

## 常见 Attention 

- [x] Self-Attention
- [x] Multi-Head Attention
- [x] Cross-Attention
- [x] Causal Attention (Masked Self-Attention)
- [x] Multi-Query Attention (MQA)
- [x] Grouped Query Attention (GQA)
- [x] Gated Attention
- [ ] Multi-Head Latent Attention (MLA)
- [x] Rotary Position Embedding (RoPE)
- [x] Sinusoidal Position Embedding
- [x] KV Cache
- [ ] Flash Attention

## 常见 RL 方法

- [ ] SFT (Supervised Fine-Tuning)
- [x] LoRA (Low-Rank Adaptation)
- [ ] Distillation
- [ ] PPO (Proximal Policy Optimization)
- [ ] DPO (Direct Preference Optimization)
- [ ] GRPO 
- [ ] SPO 
- [ ] DAPO 

## 常见 Utils

- [x] Softmax
- [x] LayerNorm
- [x] RMSNorm
- [x] SwiGLU
- [x] AdamW
- [x] Learning Rate Scheduler
- [x] Gradient Clipping
- [ ] Mixed Precision Training
- [ ] Distributed Data Parallel (DDP)

