# LLM-HandCoding-Interview
收集为大模型面试准备的手撕代码

## 常见 Attention 

- [x] Self-Attention
- [x] Multi-Head Attention
- [x] Cross-Attention
- [ ] Causal Attention (Masked Self-Attention)
- [ ] Sparse Attention
- [ ] Linear Attention
- [ ] Flash Attention
- [ ] Grouped Query Attention (GQA)
- [ ] Multi-Query Attention (MQA)
- [ ] Sliding Window Attention
- [ ] Relative Position Attention
- [ ] Rotary Position Embedding (RoPE)
- [ ] ALiBi (Attention with Linear Biases)

## 常见 RL 方法

- [ ] SFT (Supervised Fine-Tuning)
- [ ] LoRA (Low-Rank Adaptation)
- [ ] Distillation
- [ ] PPO (Proximal Policy Optimization)
- [ ] DPO (Direct Preference Optimization)
- [ ] GRPO 
- [ ] SPO
- [ ] DAPO

## 常见 Utils

- [ ] Softmax
- [ ] LayerNorm
- [ ] RMSNorm
- [ ] SwiGLU
- [ ] AdamW
- [ ] Learning Rate Scheduler
- [ ] Gradient Clipping
- [ ] Mixed Precision Training
- [ ] Distributed Data Parallel (DDP)

