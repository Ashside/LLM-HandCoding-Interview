# LLM-HandCoding-Interview
收集为大模型面试准备的手撕代码

## 常见 Attention 变种

- [ ] Self-Attention
- [ ] Multi-Head Attention
- [ ] Cross-Attention
- [ ] Scaled Dot-Product Attention
- [ ] Causal Attention (Masked Self-Attention)
- [ ] Sparse Attention
- [ ] Linear Attention
- [ ] Flash Attention
- [ ] Grouped Query Attention (GQA)
- [ ] Multi-Query Attention (MQA)
- [ ] Sliding Window Attention
- [ ] Relative Position Attention
- [ ] Rotary Position Embedding (RoPE)
- [ ] ALiBi (Attention with Linear Biases)
